/**
 * ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
 * VoiceAgent v5 ‚Äî Intelligent Barge-In Voice Controller
 *
 * SINGLE voice source for the entire app. All other voice
 * inputs removed. This is the ONLY listening mechanism.
 *
 * Architecture:
 *   ‚Ä¢ Web Speech API with continuous=true for barge-in
 *   ‚Ä¢ Recognition runs EVEN while TTS plays ‚Üí user can interrupt
 *   ‚Ä¢ When user speaks during TTS ‚Üí TTS cancelled ‚Üí listen
 *   ‚Ä¢ Gemini 2.5 Flash for ALL intelligent responses
 *   ‚Ä¢ Fast keyword shortcuts for instant navigation (0ms)
 *   ‚Ä¢ Conversation history for multi-turn context
 *   ‚Ä¢ Auto-restart after every exchange (never stops)
 *
 * Like talking to a real person:
 *   Agent speaks ‚Üí user interrupts ‚Üí agent stops & listens
 *   User pauses ‚Üí agent responds ‚Üí keeps listening
 * ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
 */

import { useState, useCallback, useRef, useEffect, memo } from 'react';
import VoiceContext from './VoiceContext';
import { sendTextToGemini, stopSpeaking } from '../utils/geminiVoiceAgent';

// ‚îÄ‚îÄ Speech lang codes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
const SPEECH_LANGS = {
    en: 'en-IN', hi: 'hi-IN', pa: 'pa-IN', bn: 'bn-IN',
    ta: 'ta-IN', te: 'te-IN', kn: 'kn-IN', mr: 'mr-IN',
};

// ‚îÄ‚îÄ TTS lang codes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
const TTS_LANGS = {
    en: 'en-IN', hi: 'hi-IN', pa: 'pa-IN', bn: 'bn-IN',
    ta: 'ta-IN', te: 'te-IN', kn: 'kn-IN', mr: 'mr-IN',
};

// ‚îÄ‚îÄ Quick nav keywords (instant, no API call) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
const NAV_KEYWORDS = {
    electricity: { words: ['bijli', 'electricity', 'electric', '‡§¨‡§ø‡§ú‡§≤‡•Ä', '‡®¨‡®ø‡®ú‡®≤‡©Ä', 'light'], route: '/bill/electricity' },
    water: { words: ['paani', 'water', 'jal', '‡§™‡§æ‡§®‡•Ä', '‡®™‡®æ‡®£‡©Ä', 'pani'], route: '/bill/water' },
    gas: { words: ['gas', '‡§ó‡•à‡§∏', '‡®ó‡©à‡®∏', 'lpg'], route: '/bill/gas' },
    property: { words: ['property', 'tax', 'ghar', '‡§™‡•ç‡§∞‡•â‡§™‡§∞‡•ç‡§ü‡•Ä', '‡®ú‡®æ‡®á‡®¶‡®æ‡®¶'], route: '/bill/electricity' },
    complaint: { words: ['complaint', 'shikayat', '‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§', '‡®∏‡®º‡®ø‡®ï‡®æ‡®á‡®§', 'problem'], route: '/complaint' },
    home: { words: ['home', 'ghar', 'shuru', '‡§π‡•ã‡§Æ', '‡®π‡©ã‡®Æ', 'main page'], route: '/' },
    back: { words: ['back', 'peeche', 'wapas', '‡§™‡•Ä‡§õ‡•á', '‡®™‡®ø‡©±‡®õ‡©á'], route: '__BACK__' },
};

function detectQuickNav(text) {
    const lower = text.toLowerCase();
    for (const [key, { words, route }] of Object.entries(NAV_KEYWORDS)) {
        if (words.some(w => lower.includes(w))) return { key, route };
    }
    return null;
}

function isStopCommand(text) {
    const stops = ['stop', 'band', 'ruko', 'bas', '‡§¨‡§Ç‡§¶', '‡§∞‡•Å‡§ï‡•ã', '‡®¨‡©∞‡®¶', '‡®∞‡©Å‡®ï‡©ã', 'chup'];
    return stops.some(s => text.toLowerCase().includes(s));
}

// ‚îÄ‚îÄ Barge-in TTS with interrupt detection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
function smartSpeak(text, lang = 'en') {
    return new Promise((resolve) => {
        if (!window.speechSynthesis || !text) { resolve(); return; }
        window.speechSynthesis.cancel();

        const u = new SpeechSynthesisUtterance(text);
        u.lang = TTS_LANGS[lang] || 'en-IN';
        u.rate = 1.05; // Slightly faster for naturalness
        u.pitch = 1;
        u.volume = 1;

        const voices = window.speechSynthesis.getVoices();
        const v = voices.find(v => v.lang === u.lang) || voices.find(v => v.lang.startsWith(lang));
        if (v) u.voice = v;

        u.onend = () => resolve();
        u.onerror = () => resolve();
        window.speechSynthesis.speak(u);
    });
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// ‚ïê‚ïê‚ïê MAIN COMPONENT ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

const VoiceAgent = memo(function VoiceAgent({
    lang, setLang, screen, setScreen,
    navigate, setCitizen, addLog, children,
}) {
    const [isActive, setIsActive] = useState(false);
    const [status, setStatus] = useState('idle');
    const [lastTranscript, setLastTranscript] = useState('');
    const [lastReply, setLastReply] = useState('');
    const [interimText, setInterimText] = useState('');

    // Refs for real-time state access in callbacks
    const isActiveRef = useRef(false);
    const isSpeakingRef = useRef(false);
    const recognitionRef = useRef(null);
    const langRef = useRef(lang);
    const screenRef = useRef(screen);
    const processingRef = useRef(false);
    const silenceTimerRef = useRef(null);

    useEffect(() => { langRef.current = lang; }, [lang]);
    useEffect(() => { screenRef.current = screen; }, [screen]);

    // ‚îÄ‚îÄ CORE: Start continuous recognition ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const startRecognition = useCallback(() => {
        const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SR) { addLog?.('Voice: not supported'); return; }

        // Kill existing
        try { recognitionRef.current?.abort(); } catch { }

        const r = new SR();
        r.lang = SPEECH_LANGS[langRef.current] || 'hi-IN';
        r.continuous = true;       // NEVER stops ‚Äî key for barge-in
        r.interimResults = true;   // Real-time transcription
        r.maxAlternatives = 1;

        r.onresult = (e) => {
            const lastResult = e.results[e.results.length - 1];

            // ‚îÄ‚îÄ BARGE-IN: User spoke while agent is speaking ‚îÄ‚îÄ
            if (isSpeakingRef.current) {
                // User is interrupting ‚Äî stop TTS immediately
                window.speechSynthesis.cancel();
                isSpeakingRef.current = false;
                setStatus('listening');
                addLog?.('üîá Barge-in: user interrupted');
            }

            if (lastResult.isFinal) {
                const transcript = lastResult[0].transcript.trim();
                if (transcript.length > 1) {
                    setInterimText('');
                    clearTimeout(silenceTimerRef.current);
                    handleFinalTranscript(transcript);
                }
            } else {
                // Show interim text for responsiveness
                setInterimText(lastResult[0].transcript);
                setStatus('listening');

                // Debounce: wait 1.5s of silence after last interim before considering done
                clearTimeout(silenceTimerRef.current);
            }
        };

        r.onerror = (e) => {
            if (e.error === 'no-speech' || e.error === 'aborted') {
                // Normal ‚Äî restart silently
                if (isActiveRef.current) setTimeout(() => startRecognition(), 300);
                return;
            }
            addLog?.(`Voice error: ${e.error}`);
            // Try restart
            if (isActiveRef.current) setTimeout(() => startRecognition(), 1000);
        };

        r.onend = () => {
            // Chrome kills continuous recognition sometimes ‚Äî restart
            if (isActiveRef.current) {
                setTimeout(() => startRecognition(), 200);
            }
        };

        recognitionRef.current = r;
        try { r.start(); } catch { }
        if (!isSpeakingRef.current) setStatus('listening');
    }, [addLog]);

    // ‚îÄ‚îÄ Process final transcript ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const handleFinalTranscript = useCallback(async (transcript) => {
        if (processingRef.current) return;
        processingRef.current = true;

        setLastTranscript(transcript);
        setStatus('processing');
        addLog?.(`üé§ "${transcript}"`);

        // Check stop command
        if (isStopCommand(transcript)) {
            await respondAndListen(
                langRef.current === 'hi'
                    ? '‡§†‡•Ä‡§ï ‡§π‡•à, ‡§¨‡§Ç‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§ ‡§ú‡§¨ ‡§ö‡§æ‡§π‡•á‡§Ç ‡§Æ‡§æ‡§á‡§ï ‡§¶‡§¨‡§æ‡§è‡§Ç‡•§'
                    : 'Okay, stopping. Tap the mic when ready.',
                true // shouldStop
            );
            processingRef.current = false;
            return;
        }

        // Check quick navigation (0ms)
        const nav = detectQuickNav(transcript);

        // ALWAYS send to Gemini for intelligent response
        // But if nav detected, also navigate
        try {
            const gemini = await sendTextToGemini(
                transcript,
                langRef.current,
                `${screenRef.current} | path: ${window.location.pathname}`
            );

            if (gemini.language && gemini.language !== langRef.current) {
                // Count detections for auto-switch
                // (Gemini detects language automatically)
            }

            const reply = gemini.reply || '';

            // Execute navigation
            if (nav) {
                if (nav.route === '__BACK__') {
                    navigate(-1);
                } else {
                    navigate(nav.route);
                }
                addLog?.(`üìç Nav ‚Üí ${nav.route}`);
            } else if (gemini.intent === 'navigate' && gemini.action_key) {
                // Gemini also detected navigation
                const routes = {
                    'electricity': '/bill/electricity', 'water': '/bill/water',
                    'gas': '/bill/gas', 'complaint': '/complaint',
                    'quick_pay': '__GUEST__', 'citizen_login': '__CITIZEN__',
                    'go_back': '__BACK__',
                };
                const r = routes[gemini.action_key];
                if (r === '__GUEST__') { setScreen('guest'); navigate('/'); }
                else if (r === '__CITIZEN__') { setScreen('citizen-auth'); }
                else if (r === '__BACK__') { navigate(-1); }
                else if (r) { navigate(r); }
                addLog?.(`üìç Gemini nav ‚Üí ${gemini.action_key}`);
            } else if (gemini.intent === 'set_screen') {
                if (gemini.action_key === 'quick_pay') { setScreen('guest'); navigate('/'); }
                else if (gemini.action_key === 'citizen_login') { setScreen('citizen-auth'); }
            }

            // Speak response (recognition stays running for barge-in)
            await respondAndListen(reply, false);
        } catch (err) {
            console.error('Gemini error:', err);
            // Fallback response
            await respondAndListen(
                langRef.current === 'hi'
                    ? '‡§Æ‡§æ‡§´‡§º ‡§ï‡•Ä‡§ú‡§ø‡§è, ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§¨‡•ã‡§≤‡•á‡§Ç‡•§'
                    : 'Sorry, please try again.',
                false
            );
        }

        processingRef.current = false;
    }, [navigate, setScreen, addLog]);

    // ‚îÄ‚îÄ Speak response while keeping recognition alive ‚îÄ‚îÄ
    const respondAndListen = useCallback(async (text, shouldStop = false) => {
        if (!text) { setStatus('listening'); return; }

        setLastReply(text);
        setStatus('speaking');
        isSpeakingRef.current = true;

        // Speak ‚Äî recognition is STILL running (barge-in ready)
        await smartSpeak(text, langRef.current);

        isSpeakingRef.current = false;

        if (shouldStop) {
            deactivateVoice();
        } else if (isActiveRef.current) {
            setStatus('listening');
        }
    }, []);

    // ‚îÄ‚îÄ ACTIVATE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const activateVoice = useCallback(() => {
        if (isActiveRef.current) return;
        isActiveRef.current = true;
        setIsActive(true);
        setLastTranscript('');
        setLastReply('');
        setInterimText('');
        stopSpeaking();
        addLog?.('üü¢ Voice agent ON');

        // Start recognition FIRST (barge-in ready from the start)
        startRecognition();

        // Then greet
        const greetings = {
            gateway: {
                hi: '‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§¨‡§§‡§æ‡§á‡§è, ‡§Ü‡§™‡§ï‡§æ ‡§Ö‡§™‡§®‡§æ ‡§¨‡§ø‡§≤ ‡§π‡•à ‡§Ø‡§æ ‡§ï‡§ø‡§∏‡•Ä ‡§∞‡§ø‡§∂‡•ç‡§§‡•á‡§¶‡§æ‡§∞ ‡§ï‡§æ?',
                en: 'Hello! Tell me, is the bill in your name or a relative\'s?',
                pa: '‡®∏‡®§ ‡®∏‡©ç‡®∞‡©Ä ‡®Ö‡®ï‡®æ‡®≤! ‡®¶‡©±‡®∏‡©ã, ‡®¨‡®ø‡©±‡®≤ ‡®§‡©Å‡®π‡®æ‡®°‡®æ ‡®π‡©à ‡®ú‡®æ‡®Ç ‡®ï‡®ø‡®∏‡©á ‡®¶‡®æ?',
            },
            guest: {
                hi: '‡§¨‡•ã‡§≤‡§ø‡§è, ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞‡§®‡§æ ‡§π‡•à? ‡§¨‡§ø‡§ú‡§≤‡•Ä, ‡§™‡§æ‡§®‡•Ä, ‡§ó‡•à‡§∏ ‡§ï‡§æ ‡§¨‡§ø‡§≤ ‡§Ø‡§æ ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§?',
                en: 'What would you like to do? Electricity, water, gas bill or complaint?',
                pa: '‡®¶‡©±‡®∏‡©ã, ‡®ï‡©Ä ‡®ï‡®∞‡®®‡®æ ‡®π‡©à? ‡®¨‡®ø‡®ú‡®≤‡©Ä, ‡®™‡®æ‡®£‡©Ä, ‡®ó‡©à‡®∏ ‡®ú‡®æ‡®Ç ‡®∏‡®º‡®ø‡®ï‡®æ‡®á‡®§?',
            },
            'citizen-dashboard': {
                hi: '‡§¨‡•ã‡§≤‡§ø‡§è, ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞‡§®‡§æ ‡§π‡•à?',
                en: 'What would you like to do?',
                pa: '‡®¶‡©±‡®∏‡©ã ‡®ï‡©Ä ‡®ï‡®∞‡®®‡®æ ‡®π‡©à?',
            },
        };
        const g = greetings[screenRef.current] || greetings.guest;
        const text = g[langRef.current] || g.en;
        respondAndListen(text, false);
    }, [startRecognition, respondAndListen, addLog]);

    // ‚îÄ‚îÄ DEACTIVATE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const deactivateVoice = useCallback(() => {
        isActiveRef.current = false;
        setIsActive(false);
        setStatus('idle');
        setInterimText('');
        isSpeakingRef.current = false;
        processingRef.current = false;
        stopSpeaking();
        clearTimeout(silenceTimerRef.current);
        try { recognitionRef.current?.abort(); } catch { }
        addLog?.('üî¥ Voice agent OFF');
    }, [addLog]);

    // Cleanup
    useEffect(() => {
        return () => {
            isActiveRef.current = false;
            try { recognitionRef.current?.abort(); } catch { }
            stopSpeaking();
        };
    }, []);

    const contextValue = {
        isActive, status, activate: activateVoice,
        deactivate: deactivateVoice, lastTranscript, lastReply,
    };

    return (
        <VoiceContext.Provider value={contextValue}>
            {children}

            {/* ‚ïê‚ïê‚ïê VOICE STATUS UI ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê */}
            {isActive && (
                <div className={`vo-bar vo-bar-${status}`}>
                    {/* Left: Status indicator */}
                    <div className="vo-bar-left">
                        {status === 'listening' && (
                            <>
                                <div className="vo-pulse" />
                                <span className="vo-bar-label">
                                    {interimText
                                        ? `"${interimText}"`
                                        : (lang === 'hi' ? '‡§¨‡•ã‡§≤‡§ø‡§è...' : 'Speak...')}
                                </span>
                            </>
                        )}
                        {status === 'processing' && (
                            <>
                                <div className="vo-spinner" />
                                <span className="vo-bar-label">
                                    {lang === 'hi' ? '‡§∏‡§Æ‡§ù ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å...' : 'Thinking...'}
                                </span>
                            </>
                        )}
                        {status === 'speaking' && (
                            <>
                                <div className="vo-waves">
                                    {[...Array(4)].map((_, i) => (
                                        <div key={i} className="vo-wave" style={{ animationDelay: `${i * 0.12}s` }} />
                                    ))}
                                </div>
                                <span className="vo-bar-reply">
                                    {lastReply?.substring(0, 70)}{lastReply?.length > 70 ? '...' : ''}
                                </span>
                            </>
                        )}
                    </div>

                    {/* Right: Stop button */}
                    <button
                        className="vo-bar-close"
                        onClick={status === 'speaking'
                            ? () => { stopSpeaking(); isSpeakingRef.current = false; setStatus('listening'); }
                            : deactivateVoice
                        }
                    >
                        {status === 'speaking' ? '‚è≠' : '‚úï'}
                    </button>
                </div>
            )}
        </VoiceContext.Provider>
    );
});

export default VoiceAgent;
