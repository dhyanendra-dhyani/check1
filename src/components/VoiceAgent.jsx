/**
 * ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
 * VoiceAgent v6 ‚Äî Full Pipeline: VAD + Streaming + Barge-in
 *
 * 4 KEY UPGRADES:
 *   1. VAD (@ricky0123/vad-web) ‚Äî instant end-of-speech detection
 *   2. Barge-in ‚Äî user interrupts agent ‚Üí agent stops immediately
 *   3. Gemini Streaming ‚Äî TTS starts on first sentence
 *   4. Parallel pipeline ‚Äî STT, LLM, TTS all overlap
 *
 * Flow:
 *   [MIC] ‚Üí VAD detects speech ‚Üí Web Speech API transcribes
 *   ‚Üí VAD detects silence ‚Üí INSTANTLY send to Gemini (streaming)
 *   ‚Üí First sentence arrives ‚Üí START TTS immediately
 *   ‚Üí More sentences arrive ‚Üí QUEUE TTS
 *   ‚Üí User speaks during TTS ‚Üí BARGE-IN ‚Üí cancel TTS, listen
 *   ‚Üí Repeat forever until stopped
 * ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
 */

import { useState, useCallback, useRef, useEffect, memo } from 'react';
import VoiceContext from './VoiceContext';
import { streamGeminiResponse, stopSpeaking, speakText } from '../utils/geminiVoiceAgent';

// ‚îÄ‚îÄ Speech codes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
const SPEECH_LANGS = {
    en: 'en-IN', hi: 'hi-IN', pa: 'pa-IN', bn: 'bn-IN',
    ta: 'ta-IN', te: 'te-IN', kn: 'kn-IN', mr: 'mr-IN',
};

const TTS_LANGS = {
    en: 'en-IN', hi: 'hi-IN', pa: 'pa-IN', bn: 'bn-IN',
    ta: 'ta-IN', te: 'te-IN', kn: 'kn-IN', mr: 'mr-IN',
};

// ‚îÄ‚îÄ Quick nav keywords (instant, 0ms) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
const NAV_KEYWORDS = {
    electricity: { words: ['bijli', 'electricity', 'electric', '‡§¨‡§ø‡§ú‡§≤‡•Ä', '‡®¨‡®ø‡®ú‡®≤‡©Ä', 'light', 'bijlee'], route: '/bill/electricity' },
    water: { words: ['paani', 'water', 'jal', '‡§™‡§æ‡§®‡•Ä', '‡®™‡®æ‡®£‡©Ä', 'pani'], route: '/bill/water' },
    gas: { words: ['gas', '‡§ó‡•à‡§∏', '‡®ó‡©à‡®∏', 'lpg', 'cylinder'], route: '/bill/gas' },
    property: { words: ['property', 'tax', '‡§™‡•ç‡§∞‡•â‡§™‡§∞‡•ç‡§ü‡•Ä', '‡®ú‡®æ‡®á‡®¶‡®æ‡®¶', 'sampatti'], route: '/bill/electricity' },
    complaint: { words: ['complaint', 'shikayat', '‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§', '‡®∏‡®º‡®ø‡®ï‡®æ‡®á‡®§', 'problem', 'samasya'], route: '/complaint' },
    home: { words: ['home', 'ghar', 'shuru', '‡§π‡•ã‡§Æ', '‡®π‡©ã‡®Æ', 'main'], route: '/' },
    back: { words: ['back', 'peeche', 'wapas', '‡§™‡•Ä‡§õ‡•á', '‡®™‡®ø‡©±‡®õ‡©á'], route: '__BACK__' },
};

function detectQuickNav(text) {
    const lower = text.toLowerCase();
    for (const [, { words, route }] of Object.entries(NAV_KEYWORDS))
        if (words.some(w => lower.includes(w))) return route;
    return null;
}

function isStopCommand(text) {
    return ['stop', 'band', 'ruko', 'bas', '‡§¨‡§Ç‡§¶', '‡§∞‡•Å‡§ï‡•ã', '‡®¨‡©∞‡®¶', 'chup', 'bye']
        .some(s => text.toLowerCase().includes(s));
}

// ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
const VoiceAgent = memo(function VoiceAgent({
    lang, setLang, screen, setScreen,
    navigate, setCitizen, addLog, children,
}) {
    const [isActive, setIsActive] = useState(false);
    const [status, setStatus] = useState('idle');
    const [lastTranscript, setLastTranscript] = useState('');
    const [lastReply, setLastReply] = useState('');
    const [interimText, setInterimText] = useState('');

    const isActiveRef = useRef(false);
    const isSpeakingRef = useRef(false);
    const recognitionRef = useRef(null);
    const langRef = useRef(lang);
    const screenRef = useRef(screen);
    const processingRef = useRef(false);
    const vadRef = useRef(null);
    const silenceTimerRef = useRef(null);
    const lastInterimRef = useRef('');
    const bargedInRef = useRef(false);

    useEffect(() => { langRef.current = lang; }, [lang]);
    useEffect(() => { screenRef.current = screen; }, [screen]);

    // ‚îÄ‚îÄ Initialize VAD ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const initVAD = useCallback(async () => {
        try {
            const vadModule = await import('@ricky0123/vad-web');
            const vad = await vadModule.MicVAD.new({
                positiveSpeechThreshold: 0.8,
                negativeSpeechThreshold: 0.3,
                minSpeechFrames: 3,
                preSpeechPadFrames: 3,
                redemptionFrames: 8, // ~480ms silence = speech ended

                onSpeechStart: () => {
                    // ‚îÄ‚îÄ BARGE-IN: User started speaking ‚îÄ‚îÄ
                    if (isSpeakingRef.current) {
                        window.speechSynthesis.cancel();
                        isSpeakingRef.current = false;
                        bargedInRef.current = true;
                        setStatus('listening');
                        addLog?.('üîá Barge-in!');
                    }
                    // Clear the silence timer
                    clearTimeout(silenceTimerRef.current);
                },

                onSpeechEnd: () => {
                    // ‚îÄ‚îÄ User stopped speaking ‚Üí process IMMEDIATELY ‚îÄ‚îÄ
                    if (!isActiveRef.current || processingRef.current) return;

                    // Give Web Speech API a moment to finalize, then force-process
                    silenceTimerRef.current = setTimeout(() => {
                        const transcript = lastInterimRef.current?.trim();
                        if (transcript && transcript.length > 1 && !processingRef.current) {
                            addLog?.('üîï VAD: speech ended');
                            handleFinalTranscript(transcript);
                        }
                    }, 300); // 300ms after VAD detects silence ‚Äî much faster than default
                },
            });

            vadRef.current = vad;
            return vad;
        } catch (err) {
            console.warn('VAD init failed, using fallback:', err);
            addLog?.('‚ö†Ô∏è VAD unavailable, using timer fallback');
            return null;
        }
    }, [addLog]);

    // ‚îÄ‚îÄ Start recognition (Web Speech API for transcription) ‚îÄ‚îÄ
    const startRecognition = useCallback(() => {
        const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SR) return;

        try { recognitionRef.current?.abort(); } catch { }

        const r = new SR();
        r.lang = SPEECH_LANGS[langRef.current] || 'hi-IN';
        r.continuous = true;
        r.interimResults = true;
        r.maxAlternatives = 1;

        r.onresult = (e) => {
            const lastResult = e.results[e.results.length - 1];

            // BARGE-IN via speech recognition too
            if (isSpeakingRef.current) {
                window.speechSynthesis.cancel();
                isSpeakingRef.current = false;
                bargedInRef.current = true;
                setStatus('listening');
            }

            if (lastResult.isFinal) {
                const t = lastResult[0].transcript.trim();
                lastInterimRef.current = t;
                setInterimText('');
                if (t.length > 1 && !processingRef.current) {
                    clearTimeout(silenceTimerRef.current);
                    handleFinalTranscript(t);
                }
            } else {
                const interim = lastResult[0].transcript;
                lastInterimRef.current = interim;
                setInterimText(interim);
                if (!isSpeakingRef.current) setStatus('listening');

                // Fallback silence timer (in case VAD isn't available)
                if (!vadRef.current) {
                    clearTimeout(silenceTimerRef.current);
                    silenceTimerRef.current = setTimeout(() => {
                        const t = lastInterimRef.current?.trim();
                        if (t && t.length > 1 && !processingRef.current) {
                            handleFinalTranscript(t);
                        }
                    }, 1200);
                }
            }
        };

        r.onerror = (e) => {
            if (['no-speech', 'aborted'].includes(e.error)) {
                if (isActiveRef.current) setTimeout(() => startRecognition(), 300);
                return;
            }
            addLog?.(`Voice error: ${e.error}`);
            if (isActiveRef.current) setTimeout(() => startRecognition(), 1000);
        };

        r.onend = () => {
            if (isActiveRef.current) setTimeout(() => startRecognition(), 200);
        };

        recognitionRef.current = r;
        try { r.start(); } catch { }
        if (!isSpeakingRef.current) setStatus('listening');
    }, [addLog]);

    // ‚îÄ‚îÄ Process transcript: PARALLEL pipeline ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const handleFinalTranscript = useCallback(async (transcript) => {
        if (processingRef.current) return;
        processingRef.current = true;
        bargedInRef.current = false;

        setLastTranscript(transcript);
        setInterimText('');
        lastInterimRef.current = '';
        setStatus('processing');
        addLog?.(`üé§ "${transcript}"`);

        // Stop command?
        if (isStopCommand(transcript)) {
            await speakAndFinish(
                langRef.current === 'hi' ? '‡§†‡•Ä‡§ï ‡§π‡•à, ‡§¨‡§Ç‡§¶ ‡§ï‡§∞ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§' : 'Okay, stopping.',
                true
            );
            processingRef.current = false;
            return;
        }

        // Quick nav detection (instant, 0ms)
        const navRoute = detectQuickNav(transcript);

        try {
            // ‚îÄ‚îÄ PARALLEL: Stream Gemini + Start TTS on first sentence ‚îÄ‚îÄ
            let firstSentenceSpoken = false;
            let fullReply = '';

            const result = await streamGeminiResponse(
                transcript,
                langRef.current,
                `${screenRef.current} | ${window.location.pathname}`,
                // onSentence callback ‚Äî TTS starts HERE
                async (sentence, index) => {
                    if (bargedInRef.current) return; // User interrupted

                    fullReply += (index > 0 ? ' ' : '') + sentence;
                    setLastReply(fullReply);

                    if (index === 0) {
                        // FIRST sentence ‚Üí start TTS immediately (parallel!)
                        setStatus('speaking');
                        isSpeakingRef.current = true;
                        firstSentenceSpoken = true;

                        // Navigate NOW if detected (don't wait for TTS)
                        if (navRoute) {
                            executeNav(navRoute);
                        }
                    }

                    // Queue TTS for each sentence (they play in order)
                    if (!bargedInRef.current) {
                        await queueTTS(sentence, langRef.current);
                    }
                }
            );

            // If streaming didn't trigger TTS (maybe very short response)
            if (!firstSentenceSpoken && result.reply && !bargedInRef.current) {
                setLastReply(result.reply);
                setStatus('speaking');
                isSpeakingRef.current = true;
                await queueTTS(result.reply, langRef.current);
            }

            // Execute Gemini-detected navigation
            if (!navRoute && result.intent === 'navigate' && result.action_key) {
                const routes = {
                    electricity: '/bill/electricity', water: '/bill/water',
                    gas: '/bill/gas', complaint: '/complaint',
                };
                if (routes[result.action_key]) executeNav(routes[result.action_key]);
            } else if (result.intent === 'set_screen') {
                if (result.action_key === 'quick_pay') { setScreen('guest'); navigate('/'); }
                else if (result.action_key === 'citizen_login') { setScreen('citizen-auth'); }
            } else if (result.intent === 'go_back') {
                navigate(-1);
            } else if (navRoute && !firstSentenceSpoken) {
                // Navigate even if TTS hasn't started
                executeNav(navRoute);
            }

        } catch (err) {
            console.error('Pipeline error:', err);
            if (!bargedInRef.current) {
                await speakAndFinish(
                    langRef.current === 'hi' ? '‡§Æ‡§æ‡§´‡§º ‡§ï‡•Ä‡§ú‡§ø‡§è, ‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§ø‡§∞ ‡§∏‡•á ‡§¨‡•ã‡§≤‡•á‡§Ç‡•§'
                        : 'Sorry, please try again.',
                    false
                );
            }
        }

        // Done speaking ‚Üí back to listening
        isSpeakingRef.current = false;
        if (isActiveRef.current && !bargedInRef.current) {
            setStatus('listening');
        }
        processingRef.current = false;
    }, [navigate, setScreen, addLog]);

    // ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const executeNav = useCallback((route) => {
        if (route === '__BACK__') navigate(-1);
        else navigate(route);
        addLog?.(`üìç ‚Üí ${route}`);
    }, [navigate, addLog]);

    const queueTTS = useCallback((text, lang) => {
        return new Promise((resolve) => {
            if (!window.speechSynthesis || !text || bargedInRef.current) {
                resolve();
                return;
            }
            const u = new SpeechSynthesisUtterance(text);
            u.lang = TTS_LANGS[lang] || 'en-IN';
            u.rate = 1.05;
            u.pitch = 1;
            u.volume = 1;
            const voices = window.speechSynthesis.getVoices();
            const v = voices.find(v => v.lang === u.lang) || voices.find(v => v.lang.startsWith(lang));
            if (v) u.voice = v;
            u.onend = () => resolve();
            u.onerror = () => resolve();
            window.speechSynthesis.speak(u);
        });
    }, []);

    const speakAndFinish = useCallback(async (text, shouldStop) => {
        setLastReply(text);
        setStatus('speaking');
        isSpeakingRef.current = true;
        await speakText(text, langRef.current);
        isSpeakingRef.current = false;
        if (shouldStop) deactivateVoice();
        else if (isActiveRef.current) setStatus('listening');
    }, []);

    // ‚îÄ‚îÄ ACTIVATE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const activateVoice = useCallback(async () => {
        if (isActiveRef.current) return;
        isActiveRef.current = true;
        setIsActive(true);
        setLastTranscript('');
        setLastReply('');
        setInterimText('');
        stopSpeaking();
        addLog?.('üü¢ Voice ON');

        // Start VAD + Recognition in parallel
        const vad = await initVAD();
        if (vad) {
            try { vad.start(); addLog?.('üéØ VAD active'); }
            catch (e) { console.warn('VAD start failed:', e); }
        }
        startRecognition();

        // Greet
        const greetings = {
            gateway: {
                hi: '‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§¨‡§§‡§æ‡§á‡§è ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞‡§®‡§æ ‡§π‡•à?',
                en: 'Hello! What would you like to do?',
                pa: '‡®∏‡®§ ‡®∏‡©ç‡®∞‡©Ä ‡®Ö‡®ï‡®æ‡®≤! ‡®¶‡©±‡®∏‡©ã ‡®ï‡©Ä ‡®ï‡®∞‡®®‡®æ ‡®π‡©à?',
            },
            guest: {
                hi: '‡§¨‡•ã‡§≤‡§ø‡§è, ‡§ï‡•å‡§® ‡§∏‡§æ ‡§¨‡§ø‡§≤ ‡§≠‡§∞‡§®‡§æ ‡§π‡•à?',
                en: 'Which bill would you like to pay?',
                pa: '‡®¶‡©±‡®∏‡©ã, ‡®ï‡®ø‡®π‡©ú‡®æ ‡®¨‡®ø‡©±‡®≤ ‡®≠‡®∞‡®®‡®æ ‡®π‡©à?',
            },
            'citizen-dashboard': {
                hi: '‡§¨‡•ã‡§≤‡§ø‡§è, ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞‡§®‡§æ ‡§π‡•à?',
                en: 'What would you like to do?',
                pa: '‡®¶‡©±‡®∏‡©ã ‡®ï‡©Ä ‡®ï‡®∞‡®®‡®æ ‡®π‡©à?',
            },
        };
        const g = greetings[screenRef.current] || greetings.guest;
        const text = g[langRef.current] || g.en;
        await speakAndFinish(text, false);
    }, [initVAD, startRecognition, addLog]);

    // ‚îÄ‚îÄ DEACTIVATE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    const deactivateVoice = useCallback(() => {
        isActiveRef.current = false;
        setIsActive(false);
        setStatus('idle');
        setInterimText('');
        isSpeakingRef.current = false;
        processingRef.current = false;
        stopSpeaking();
        clearTimeout(silenceTimerRef.current);
        try { recognitionRef.current?.abort(); } catch { }
        try { vadRef.current?.pause(); } catch { }
        addLog?.('üî¥ Voice OFF');
    }, [addLog]);

    useEffect(() => {
        return () => {
            isActiveRef.current = false;
            try { recognitionRef.current?.abort(); } catch { }
            try { vadRef.current?.destroy(); } catch { }
            stopSpeaking();
        };
    }, []);

    const ctx = {
        isActive, status, activate: activateVoice,
        deactivate: deactivateVoice, lastTranscript, lastReply,
    };

    return (
        <VoiceContext.Provider value={ctx}>
            {children}

            {/* ‚ïê‚ïê‚ïê VOICE STATUS BAR ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê */}
            {isActive && (
                <div className={`vo-bar vo-bar-${status}`}>
                    <div className="vo-bar-left">
                        {status === 'listening' && (
                            <>
                                <div className="vo-pulse" />
                                <span className="vo-bar-label">
                                    {interimText
                                        ? `"${interimText}"`
                                        : (lang === 'hi' ? '‡§¨‡•ã‡§≤‡§ø‡§è...' : 'Speak...')}
                                </span>
                            </>
                        )}
                        {status === 'processing' && (
                            <>
                                <div className="vo-spinner" />
                                <span className="vo-bar-label">
                                    {lang === 'hi' ? '‡§∏‡§Æ‡§ù ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å...' : 'Thinking...'}
                                </span>
                            </>
                        )}
                        {status === 'speaking' && (
                            <>
                                <div className="vo-waves">
                                    {[...Array(4)].map((_, i) => (
                                        <div key={i} className="vo-wave" style={{ animationDelay: `${i * 0.12}s` }} />
                                    ))}
                                </div>
                                <span className="vo-bar-reply">
                                    {lastReply?.substring(0, 80)}{lastReply?.length > 80 ? '...' : ''}
                                </span>
                            </>
                        )}
                    </div>
                    <button className="vo-bar-close"
                        onClick={status === 'speaking'
                            ? () => { stopSpeaking(); isSpeakingRef.current = false; bargedInRef.current = true; setStatus('listening'); }
                            : deactivateVoice
                        }>
                        {status === 'speaking' ? '‚è≠' : '‚úï'}
                    </button>
                </div>
            )}
        </VoiceContext.Provider>
    );
});

export default VoiceAgent;
