/**
 * ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
 * VoiceAgent v14 ‚Äî Production-Quality Voice Assistant
 *
 * ROOT CAUSE OF BARGE-IN NOT WORKING:
 * Web SpeechRecognition picks up TTS audio through the mic
 * and processes it as the user speaking. This causes:
 * - False transcripts during agent speech
 * - No real barge-in (recognition is disabled on most browsers
 *   by default when TTS is active anyway)
 *
 * SOLUTION: Pause STT when TTS starts, restart immediately when
 * user speaks. Use a "barge-in detection window" ‚Äî keep STT
 * running but with a very tight energy threshold.
 *
 * ACTUALLY: The real WebSpeech API fix is:
 * 1. Stop recognition when TTS starts
 * 2. Monitor for onaudiostart on a parallel recognition instance
 * 3. Or: use onspeechstart to detect user speech during TTS
 *
 * PRACTICAL APPROACH (works on Chrome/Edge):
 * - Single recognition, continuous mode
 * - During TTS: if we get final transcript with length > 3 chars,
 *   it's real barge-in (TTS echo is usually < 3 chars or matches
 *   what we're saying)
 * - Cancel TTS + process transcript
 *
 * ALSO: Post-auth intent routing ‚Äî store what user wanted before
 * auth, navigate there after login succeeds.
 * ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
 */

import { useState, useCallback, useRef, useEffect, memo } from 'react';
import { useLocation } from 'react-router-dom';
import VoiceContext from './VoiceContext';
import { streamGeminiResponse, stopSpeaking, hasApiKeys } from '../utils/geminiVoiceAgent';
import {
    CONV_STATES, CITIZEN_KEYWORDS, GUEST_KEYWORDS,
    CITIZEN_REQUIRED_KEYWORDS, RE_PROMPT_GREETINGS,
    COMPLAINT_KEYWORDS, BACK_KEYWORDS, HOME_KEYWORDS, STOP_KEYWORDS,
    matchesKeywords, detectBillType,
    findCommonAnswer, getPageGuidance,
    getResponse, getInitialGreeting,
} from '../utils/voiceKnowledgeBase';

const SPEECH_LANGS = {
    en: 'en-IN', hi: 'hi-IN', pa: 'pa-IN', bn: 'bn-IN',
    ta: 'ta-IN', te: 'te-IN', kn: 'kn-IN', mr: 'mr-IN',
};

const SCREEN_GUIDANCE = {
    'citizen-auth': {
        hi: '‡§≤‡•â‡§ó‡§ø‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§§‡•Ä‡§® ‡§µ‡§ø‡§ï‡§≤‡•ç‡§™ ‡§π‡•à‡§Ç ‚Äî ‡§Ö‡§Ç‡§ó‡•Ç‡§†‡§æ ‡§≤‡§ó‡§æ‡§á‡§è, ‡§Ü‡§Å‡§ñ ‡§∏‡•ç‡§ï‡•à‡§® ‡§ï‡§∞‡§æ‡§á‡§è, ‡§Ø‡§æ OTP ‡§°‡§æ‡§≤‡§ø‡§è‡•§ ‡§Ö‡§Ç‡§ó‡•Ç‡§†‡§æ ‡§∏‡§¨‡§∏‡•á ‡§Ü‡§∏‡§æ‡§® ‡§π‡•à‡•§ ‡§¨‡•ã‡§≤‡•á‡§Ç "‡§Ö‡§Ç‡§ó‡•Ç‡§†‡§æ", "‡§Ü‡§Å‡§ñ", ‡§Ø‡§æ "OTP"‡•§',
        en: 'Three login options ‚Äî Thumbprint, Iris scan, or OTP. Thumbprint is easiest. Say "thumb", "iris", or "OTP".',
    },
    'citizen-dashboard': {
        hi: '‡§Ü‡§™‡§ï‡§æ ‡§°‡•à‡§∂‡§¨‡•ã‡§∞‡•ç‡§° ‡§ñ‡•Å‡§≤ ‡§ó‡§Ø‡§æ‡•§ ‡§Ø‡§π‡§æ‡§Å ‡§¨‡§ï‡§æ‡§Ø‡§æ ‡§¨‡§ø‡§≤, ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§‡•á‡§Ç, ‡§î‡§∞ ‡§∏‡•á‡§µ‡§æ‡§è‡§Ç ‡§π‡•à‡§Ç‡•§ ‡§¨‡•ã‡§≤‡•á‡§Ç ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞‡§®‡§æ ‡§π‡•à‡•§',
        en: 'Dashboard is open. Bills, complaints, and services. What would you like to do?',
    },
    guest: {
        hi: '‡§ï‡•å‡§® ‡§∏‡§æ ‡§¨‡§ø‡§≤ ‡§≠‡§∞‡§®‡§æ ‡§π‡•à ‚Äî ‡§¨‡§ø‡§ú‡§≤‡•Ä, ‡§™‡§æ‡§®‡•Ä, ‡§Ø‡§æ ‡§ó‡•à‡§∏? ‡§∂‡§ø‡§ï‡§æ‡§Ø‡§§ ‡§≠‡•Ä ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç‡•§',
        en: 'Which bill ‚Äî electricity, water, or gas? You can also file a complaint.',
    },
};

const TRANSITION_PHRASE = {
    hi: '‡§†‡•Ä‡§ï ‡§π‡•à, ',
    en: 'Alright, ',
};

const VoiceAgent = memo(function VoiceAgent({
    lang, setLang, screen, setScreen, voiceMode,
    navigate, setCitizen, addLog, children,
}) {
    const [isActive, setIsActive] = useState(false);
    const [status, setStatus] = useState('idle');
    const [lastTranscript, setLastTranscript] = useState('');
    const [lastReply, setLastReply] = useState('');
    const [interimText, setInterimText] = useState('');

    // Core refs
    const isActiveRef = useRef(false);
    const isSpeakingRef = useRef(false);
    const recognitionRef = useRef(null);
    const langRef = useRef(lang);
    const screenRef = useRef(screen);
    const processingRef = useRef(false);
    const bargedInRef = useRef(false);
    const pendingTranscriptRef = useRef(null);
    const silenceTimerRef = useRef(null);
    const lastInterimRef = useRef('');
    const lastRouteRef = useRef('');
    const lastScreenRef = useRef(screen);
    const restartCountRef = useRef(0);
    const convStateRef = useRef(CONV_STATES.INITIAL);
    const rePromptTimerRef = useRef(null);
    const rePromptCountRef = useRef(0);
    const postAuthIntentRef = useRef(null); // what to do after citizen login

    const location = useLocation();

    useEffect(() => { langRef.current = lang; }, [lang]);
    useEffect(() => { screenRef.current = screen; }, [screen]);

    const log = useCallback((msg) => {
        console.log(`[VA] ${msg}`);
        addLog?.(msg);
    }, [addLog]);

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // TTS ‚Äî Pause STT during speech, restart immediately
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    const ttsSpeak = useCallback((text, langCode) => {
        return new Promise((resolve) => {
            if (!window.speechSynthesis || !text) { resolve(); return; }

            // Cancel any ongoing speech
            window.speechSynthesis.cancel();

            // PAUSE STT during TTS to prevent mic picking up TTS audio
            // But keep recognition object alive ‚Äî just abort the session
            try { recognitionRef.current?.abort(); } catch { }

            const u = new SpeechSynthesisUtterance(text);
            u.lang = SPEECH_LANGS[langCode] || 'hi-IN';
            u.rate = 1.05; u.pitch = 1; u.volume = 1;

            // Try to find a matching voice
            const voices = window.speechSynthesis.getVoices();
            const v = voices.find(v => v.lang === u.lang) ||
                voices.find(v => v.lang.startsWith(langCode)) ||
                voices[0];
            if (v) u.voice = v;

            u.onend = () => {
                isSpeakingRef.current = false;
                // RESUME STT after TTS finishes ‚Äî wait 250ms to avoid echo
                setTimeout(() => {
                    if (isActiveRef.current && !bargedInRef.current) {
                        restartRecognition();
                        setStatus('listening');
                    }
                }, 250);
                resolve();
            };

            u.onerror = () => {
                isSpeakingRef.current = false;
                setTimeout(() => {
                    if (isActiveRef.current) {
                        restartRecognition();
                        setStatus('listening');
                    }
                }, 250);
                resolve();
            };

            // Detect if user starts speaking DURING TTS ‚Üí barge-in
            u.onboundary = () => {
                // onboundary fires for each word ‚Äî if we have a pending barge-in, stop
                if (bargedInRef.current) {
                    window.speechSynthesis.cancel();
                }
            };

            isSpeakingRef.current = true;
            bargedInRef.current = false;
            setStatus('speaking');
            window.speechSynthesis.speak(u);
        });
    }, []);

    // Queued TTS (for streaming, checks barge-in between sentences)
    const queueTTS = useCallback((text, langCode) => {
        return new Promise((resolve) => {
            if (!window.speechSynthesis || !text || bargedInRef.current) { resolve(); return; }
            const u = new SpeechSynthesisUtterance(text);
            u.lang = SPEECH_LANGS[langCode] || 'hi-IN';
            u.rate = 1.05; u.pitch = 1; u.volume = 1;
            const voices = window.speechSynthesis.getVoices();
            const v = voices.find(v => v.lang === u.lang) || voices.find(v => v.lang.startsWith(langCode));
            if (v) u.voice = v;
            u.onend = () => resolve();
            u.onerror = () => resolve();
            window.speechSynthesis.speak(u);
        });
    }, []);

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // RECOGNITION ‚Äî restart after TTS
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    const restartRecognition = useCallback(() => {
        if (!isActiveRef.current) return;
        try { recognitionRef.current?.abort(); } catch { }
        setTimeout(() => startRecognition(), 100);
    }, []);

    const startRecognition = useCallback(() => {
        const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SR || !isActiveRef.current) return;
        try { recognitionRef.current?.abort(); } catch { }

        const r = new SR();
        r.lang = SPEECH_LANGS[langRef.current] || 'hi-IN';
        r.continuous = true;
        r.interimResults = true;
        r.maxAlternatives = 1;

        r.onstart = () => {
            restartCountRef.current = 0;
            if (!isSpeakingRef.current && isActiveRef.current) setStatus('listening');
        };

        r.onresult = (e) => {
            const last = e.results[e.results.length - 1];
            const transcript = last[0].transcript.trim();

            // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            // BARGE-IN DETECTION
            // If we're speaking AND user transcript is substantial
            // (> 2 chars, not just a noise), treat as barge-in
            // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if (isSpeakingRef.current && transcript.length > 2) {
                log(`üîá BARGE-IN: "${transcript}"`);
                window.speechSynthesis.cancel();
                isSpeakingRef.current = false;
                bargedInRef.current = true;
                setStatus('listening');
            }

            clearTimeout(rePromptTimerRef.current);

            if (last.isFinal) {
                if (transcript.length < 2) return;
                log(`üìù Final: "${transcript}"`);
                lastInterimRef.current = '';
                setInterimText('');
                clearTimeout(silenceTimerRef.current);

                if (processingRef.current) {
                    // Queue it ‚Äî process after current finishes
                    log('üì• Queued transcript');
                    pendingTranscriptRef.current = transcript;
                } else {
                    handleTranscript(transcript);
                }
            } else {
                if (transcript.length > 1) {
                    lastInterimRef.current = transcript;
                    setInterimText(transcript);
                    if (!isSpeakingRef.current) setStatus('listening');
                }

                clearTimeout(silenceTimerRef.current);
                silenceTimerRef.current = setTimeout(() => {
                    const t = lastInterimRef.current?.trim();
                    if (t && t.length > 2 && !processingRef.current && !isSpeakingRef.current) {
                        log(`‚è±Ô∏è Silence: "${t}"`);
                        handleTranscript(t);
                        lastInterimRef.current = '';
                        setInterimText('');
                    }
                }, 1400);
            }
        };

        r.onspeechstart = () => {
            // User started speaking ‚Äî if agent is speaking, this is barge-in
            if (isSpeakingRef.current) {
                log('üîá onspeechstart barge-in');
                window.speechSynthesis.cancel();
                isSpeakingRef.current = false;
                bargedInRef.current = true;
                setStatus('listening');
            }
        };

        r.onerror = (e) => {
            if (['no-speech', 'aborted'].includes(e.error)) {
                if (isActiveRef.current && !isSpeakingRef.current) {
                    restartCountRef.current++;
                    if (restartCountRef.current < 50) setTimeout(() => startRecognition(), Math.min(300 * restartCountRef.current, 2000));
                }
                return;
            }
            if (isActiveRef.current && restartCountRef.current < 50) {
                restartCountRef.current++;
                setTimeout(() => startRecognition(), 1000);
            }
        };

        r.onend = () => {
            // Only auto-restart if not in TTS (TTS end will restart it)
            if (isActiveRef.current && !isSpeakingRef.current && restartCountRef.current < 50) {
                restartCountRef.current++;
                setTimeout(() => startRecognition(), 200);
            }
        };

        recognitionRef.current = r;
        try { r.start(); } catch { if (isActiveRef.current) setTimeout(() => startRecognition(), 500); }
    }, [log]);

    // ‚ïê‚ïê‚ïê RE-PROMPT ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    const startRePromptTimer = useCallback(() => {
        clearTimeout(rePromptTimerRef.current);
        if (!isActiveRef.current || convStateRef.current !== CONV_STATES.WAIT_PATH) return;

        rePromptTimerRef.current = setTimeout(async () => {
            if (!isActiveRef.current || processingRef.current || isSpeakingRef.current) return;
            if (convStateRef.current !== CONV_STATES.WAIT_PATH) return;

            const idx = Math.min(rePromptCountRef.current, RE_PROMPT_GREETINGS.length - 1);
            const rp = RE_PROMPT_GREETINGS[idx];
            const text = rp[langRef.current] || rp.en;

            log(`üîÅ Re-prompt #${rePromptCountRef.current + 1}`);
            setLastReply(text);
            await ttsSpeak(text, langRef.current);
            rePromptCountRef.current++;
            if (isActiveRef.current && rePromptCountRef.current < 3) startRePromptTimer();
        }, 11000);
    }, [log, ttsSpeak]);

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // HANDLE TRANSCRIPT ‚Äî Main intelligence
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    const handleTranscript = useCallback(async (transcript) => {
        if (processingRef.current) return;
        processingRef.current = true;
        bargedInRef.current = false;
        pendingTranscriptRef.current = null;
        clearTimeout(rePromptTimerRef.current);

        setLastTranscript(transcript);
        setInterimText('');
        lastInterimRef.current = '';
        setStatus('processing');
        log(`üé§ [${convStateRef.current}] "${transcript}"`);

        const L = langRef.current;
        const lower = transcript.toLowerCase();

        const say = async (text) => {
            if (!text || bargedInRef.current) return;
            setLastReply(text);
            await ttsSpeak(text, L);
        };

        const finalize = () => {
            processingRef.current = false;
            isSpeakingRef.current = false;
            // Check for queued transcript from barge-in
            const pending = pendingTranscriptRef.current;
            if (pending && isActiveRef.current) {
                pendingTranscriptRef.current = null;
                log(`üì§ Processing pending: "${pending}"`);
                setTimeout(() => handleTranscript(pending), 50);
                return;
            }
            if (isActiveRef.current) setStatus('listening');
        };

        // ‚îÄ‚îÄ STOP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if (matchesKeywords(transcript, STOP_KEYWORDS)) {
            await say(getResponse('stopping', L));
            deactivateVoice();
            finalize();
            return;
        }

        // ‚îÄ‚îÄ BACK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if (matchesKeywords(transcript, BACK_KEYWORDS)) {
            navigate(-1);
            finalize();
            return;
        }

        // ‚îÄ‚îÄ HOME ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if (matchesKeywords(transcript, HOME_KEYWORDS)) {
            navigate('/');
            finalize();
            return;
        }

        // ‚îÄ‚îÄ AUTH SCREEN COMMANDS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if (screenRef.current === 'citizen-auth') {
            if (lower.includes('angootha') || lower.includes('thumb') || lower.includes('‡§Ö‡§Ç‡§ó‡•Ç‡§†‡§æ') || lower.includes('ungali') || lower.includes('finger')) {
                await say(L === 'hi'
                    ? '‡§®‡•Ä‡§ö‡•á "Thumb" ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç ‡§î‡§∞ ‡§¨‡§æ‡§Ø‡•ã‡§Æ‡•á‡§ü‡•ç‡§∞‡§ø‡§ï ‡§Æ‡§∂‡•Ä‡§® ‡§™‡§∞ ‡§Ö‡§Ç‡§ó‡•Ç‡§†‡§æ ‡§∞‡§ñ‡•á‡§Ç‡•§ 2-3 ‡§∏‡•á‡§ï‡§Ç‡§° ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§ï‡•à‡§® ‡§π‡•ã ‡§ú‡§æ‡§è‡§ó‡§æ‡•§'
                    : 'Press "Thumb" below and place your finger on the scanner. Done in 2-3 seconds.');
                finalize(); return;
            }
            if (lower.includes('aankh') || lower.includes('iris') || lower.includes('‡§Ü‡§Å‡§ñ') || lower.includes('eye') || lower.includes('ankh')) {
                await say(L === 'hi'
                    ? '"Iris" ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç ‡§î‡§∞ ‡§ï‡•à‡§Æ‡§∞‡•á ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§ñ‡•á‡§Ç, ‡§Ü‡§Å‡§ñ ‡§ñ‡•Å‡§≤‡•Ä ‡§∞‡§ñ‡•á‡§Ç‡•§'
                    : 'Press "Iris" and look at the camera with your eye open.');
                finalize(); return;
            }
            if (lower.includes('otp') || lower.includes('‡§ì‡§ü‡•Ä‡§™‡•Ä') || lower.includes('mobile') || lower.includes('‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤')) {
                await say(L === 'hi'
                    ? '"OTP" ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç‡•§ ‡§Æ‡•ã‡§¨‡§æ‡§á‡§≤ ‡§®‡§Ç‡§¨‡§∞ ‡§°‡§æ‡§≤‡•á‡§Ç‡•§ ‡§°‡•á‡§Æ‡•ã OTP ‡§π‡•à 482916‡•§'
                    : 'Press "OTP". Enter your mobile number. Demo OTP is 482916.');
                finalize(); return;
            }
        }

        // ‚îÄ‚îÄ CITIZEN-REQUIRED FEATURES ‚Üí Smart redirect ‚îÄ
        // Detect intent, store it, redirect to auth, after login go there
        const hasNaam = lower.includes('naam') || lower.includes('name') || lower.includes('‡§®‡§æ‡§Æ ‡§¨‡§¶‡§≤');
        const hasPipeline = lower.includes('pipeline') || lower.includes('gas line') || lower.includes('‡§™‡§æ‡§á‡§™‡§≤‡§æ‡§á‡§®');
        const hasConnection = (lower.includes('naya') || lower.includes('‡§®‡§Ø‡§æ')) && (lower.includes('connection') || lower.includes('‡§ï‡§®‡•á‡§ï‡•ç‡§∂‡§®'));
        const hasCert = lower.includes('certificate') || lower.includes('pramanpatra') || lower.includes('‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡§™‡§§‡•ç‡§∞');
        const hasDashboard = lower.includes('dashboard') || lower.includes('history') || lower.includes('record') || lower.includes('‡§á‡§§‡§ø‡§π‡§æ‡§∏');
        const hasSubsidy = lower.includes('subsidy') || lower.includes('‡§∏‡§¨‡•ç‡§∏‡§ø‡§°‡•Ä');

        if (hasNaam || hasPipeline || hasConnection || hasCert || hasDashboard || hasSubsidy ||
            matchesKeywords(transcript, CITIZEN_REQUIRED_KEYWORDS)) {

            log('üîê Citizen-required feature, storing intent');

            // Store post-auth intent for routing after login
            if (hasNaam) postAuthIntentRef.current = { type: 'naam_change' };
            else if (hasPipeline) postAuthIntentRef.current = { type: 'pipeline' };
            else if (hasConnection) postAuthIntentRef.current = { type: 'new_connection' };
            else if (hasCert) postAuthIntentRef.current = { type: 'certificate' };
            else postAuthIntentRef.current = { type: 'dashboard' };

            let msg = '';
            if (hasNaam) {
                msg = L === 'hi'
                    ? '‡§Ö‡§ö‡•ç‡§õ‡§æ, ‡§®‡§æ‡§Æ ‡§¨‡§¶‡§≤‡§µ‡§æ‡§®‡§æ ‡§π‡•à! ‡§á‡§∏‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§ß‡§æ‡§∞ ‡§∏‡•á ‡§≤‡•â‡§ó‡§ø‡§® ‡§ï‡§∞‡§®‡§æ ‡§π‡•ã‡§ó‡§æ‡•§ ‡§Ö‡§Ç‡§ó‡•Ç‡§†‡§æ ‡§≤‡§ó‡§æ‡§á‡§è ‡§Ø‡§æ OTP ‡§°‡§æ‡§≤‡§ø‡§è ‚Äî ‡§¨‡§∏ 2-3 ‡§∏‡•á‡§ï‡§Ç‡§° ‡§Æ‡•á‡§Ç‡•§ ‡§Æ‡•à‡§Ç ‡§≤‡•â‡§ó‡§ø‡§® ‡§™‡•á‡§ú ‡§™‡§∞ ‡§≤‡•á ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§'
                    : 'Name change! You need Aadhaar login for this. Thumbprint or OTP ‚Äî just 2-3 seconds. Taking you to login.';
            } else if (hasPipeline) {
                msg = L === 'hi'
                    ? '‡§ó‡•à‡§∏ ‡§™‡§æ‡§á‡§™‡§≤‡§æ‡§á‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§ß‡§æ‡§∞ ‡§≤‡•â‡§ó‡§ø‡§® ‡§ú‡§º‡§∞‡•Ç‡§∞‡•Ä ‡§π‡•à‡•§ ‡§Ö‡§™‡§®‡•á ‡§®‡§æ‡§Æ ‡§∏‡•á ‡§ï‡§∞‡§æ‡§®‡§æ ‡§π‡•à ‡§§‡•ã ‡§Ö‡§Ç‡§ó‡•Ç‡§†‡§æ ‡§≤‡§ó‡§æ‡§á‡§è ‚Äî ‡§¨‡§π‡•Å‡§§ ‡§Ü‡§∏‡§æ‡§® ‡§π‡•à‡•§'
                    : 'Gas pipeline needs Aadhaar login. Just thumbprint if it\'s in your name ‚Äî very easy.';
            } else if (hasConnection) {
                msg = L === 'hi'
                    ? '‡§®‡§Ø‡§æ ‡§ï‡§®‡•á‡§ï‡•ç‡§∂‡§® ‡§≤‡§ó‡§µ‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§ß‡§æ‡§∞ ‡§≤‡•â‡§ó‡§ø‡§® ‡§ö‡§æ‡§π‡§ø‡§è‡•§ ‡§Ö‡§Ç‡§ó‡•Ç‡§†‡§æ ‡§≤‡§ó‡§æ‡§á‡§è‡•§'
                    : 'New connection needs Aadhaar login. Place your thumb.';
            } else {
                msg = L === 'hi'
                    ? '‡§á‡§∏ ‡§∏‡•á‡§µ‡§æ ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§ß‡§æ‡§∞ ‡§∏‡•á ‡§≤‡•â‡§ó‡§ø‡§® ‡§ï‡§∞‡§®‡§æ ‡§π‡•ã‡§ó‡§æ‡•§ ‡§Æ‡•à‡§Ç ‡§≤‡•â‡§ó‡§ø‡§® ‡§™‡•á‡§ú ‡§™‡§∞ ‡§≤‡•á ‡§ú‡§æ ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å‡•§'
                    : 'This service needs Aadhaar login. Taking you to the login page.';
            }

            await say(msg);
            convStateRef.current = CONV_STATES.CITIZEN_AUTH;
            setScreen('citizen-auth');
            finalize();
            return;
        }

        // ‚îÄ‚îÄ COMMON Q&A (instant) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        const qa = findCommonAnswer(transcript, L);
        if (qa) {
            log('üìö Q&A match');
            await say(qa);
            finalize();
            return;
        }

        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
        // STATE MACHINE
        // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

        const state = convStateRef.current;

        if (state === CONV_STATES.WAIT_PATH || state === CONV_STATES.INITIAL) {

            // Citizen path
            if (matchesKeywords(transcript, CITIZEN_KEYWORDS)) {
                log('‚Üí Citizen path');
                convStateRef.current = CONV_STATES.CITIZEN_AUTH;
                await say(getResponse('citizen_chosen', L));
                setScreen('citizen-auth');
                finalize();
                return;
            }

            // Guest path
            if (matchesKeywords(transcript, GUEST_KEYWORDS)) {
                log('‚Üí Guest path');
                convStateRef.current = CONV_STATES.GUEST_HOME;
                setScreen('guest');
                navigate('/');
                await say(getResponse('guest_chosen', L));
                finalize();
                return;
            }

            // Direct bill mention ‚Üí guest path automatically
            const billType = detectBillType(transcript);
            if (billType) {
                log(`‚Üí Direct bill: ${billType}`);
                convStateRef.current = CONV_STATES.BILL_INPUT;
                setScreen('guest');
                navigate(`/bill/${billType}`);
                finalize();
                return;
            }

            // Direct complaint
            if (matchesKeywords(transcript, COMPLAINT_KEYWORDS)) {
                log('‚Üí Direct complaint');
                convStateRef.current = CONV_STATES.COMPLAINT_CAT;
                setScreen('guest');
                navigate('/complaint');
                finalize();
                return;
            }
        }

        // Any state: bill/complaint
        const billType = detectBillType(transcript);
        if (billType) {
            log(`‚Üí Bill: ${billType}`);
            navigate(`/bill/${billType}`);
            finalize();
            return;
        }

        if (matchesKeywords(transcript, COMPLAINT_KEYWORDS)) {
            log('‚Üí Complaint');
            navigate('/complaint');
            finalize();
            return;
        }

        // ‚îÄ‚îÄ GEMINI FALLBACK ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        if (hasApiKeys()) {
            try {
                let fullReply = '';
                let firstSent = false;

                const result = await streamGeminiResponse(
                    transcript, L, `screen:${screenRef.current} path:${window.location.pathname}`,
                    async (sentence, idx) => {
                        if (bargedInRef.current) return;
                        fullReply += (idx > 0 ? ' ' : '') + sentence;
                        setLastReply(fullReply);
                        if (idx === 0) { isSpeakingRef.current = true; setStatus('speaking'); firstSent = true; }
                        if (!bargedInRef.current) await queueTTS(sentence, L);
                    }
                );

                if (!firstSent && result.reply && !bargedInRef.current) {
                    await say(result.reply);
                }

                if (!bargedInRef.current && result.intent) {
                    if (result.intent === 'navigate' && result.action_key) {
                        const routes = { electricity: '/bill/electricity', water: '/bill/water', gas: '/bill/gas', complaint: '/complaint', home: '/' };
                        if (routes[result.action_key]) navigate(routes[result.action_key]);
                    } else if (result.intent === 'set_screen') {
                        if (result.action_key === 'quick_pay') { setScreen('guest'); navigate('/'); }
                        else if (result.action_key === 'citizen_login') setScreen('citizen-auth');
                    } else if (result.intent === 'go_back') navigate(-1);
                }

            } catch (err) {
                log(`‚ùå Gemini: ${err.message}`);
                if (!bargedInRef.current) await say(getResponse('not_understood', L));
            }
        } else {
            if (!bargedInRef.current) await say(getResponse('not_understood', L));
        }

        finalize();
    }, [navigate, setScreen, log, ttsSpeak, queueTTS]);

    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // POST-AUTH ROUTING ‚Äî after citizen login succeeds
    // ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    // App.jsx calls handleAuth which sets screen='citizen-dashboard'
    // We detect that and check postAuthIntentRef

    useEffect(() => {
        if (!isActiveRef.current || !voiceMode) return;
        if (screen === lastScreenRef.current) return;

        const prevScreen = lastScreenRef.current;
        lastScreenRef.current = screen;
        log(`üì∫ Screen: ${prevScreen} ‚Üí ${screen}`);

        if (!prevScreen || screen === 'idle') return;

        // Cancel whatever agent was saying about old screen
        window.speechSynthesis?.cancel();
        isSpeakingRef.current = false;
        bargedInRef.current = true;

        const speakForScreen = async () => {
            if (!isActiveRef.current) return;
            bargedInRef.current = false;

            // Post-auth: if user had an intent before login, guide them there
            if (screen === 'citizen-dashboard' && postAuthIntentRef.current) {
                const intent = postAuthIntentRef.current;
                postAuthIntentRef.current = null;

                let msg = '';
                const L = langRef.current;

                if (intent.type === 'naam_change') {
                    msg = L === 'hi'
                        ? '‡§≤‡•â‡§ó‡§ø‡§® ‡§π‡•ã ‡§ó‡§Ø‡§æ! ‡§Ö‡§¨ ‡§®‡§æ‡§Æ ‡§¨‡§¶‡§≤‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‚Äî ‡§°‡•à‡§∂‡§¨‡•ã‡§∞‡•ç‡§° ‡§Æ‡•á‡§Ç ‡§®‡•Ä‡§ö‡•á "‚úèÔ∏è Name Change" ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç‡•§'
                        : 'Logged in! For name change ‚Äî press "‚úèÔ∏è Name Change" button below on the dashboard.';
                } else if (intent.type === 'pipeline') {
                    msg = L === 'hi'
                        ? '‡§≤‡•â‡§ó‡§ø‡§® ‡§π‡•ã ‡§ó‡§Ø‡§æ! ‡§ó‡•à‡§∏ ‡§™‡§æ‡§á‡§™‡§≤‡§æ‡§á‡§® ‡§ï‡•á ‡§≤‡§ø‡§è ‡§°‡•à‡§∂‡§¨‡•ã‡§∞‡•ç‡§° ‡§Æ‡•á‡§Ç "Apply New Connection" ‡§¶‡•á‡§ñ‡•á‡§Ç‡•§'
                        : 'Logged in! For gas pipeline, check "Apply New Connection" on the dashboard.';
                } else if (intent.type === 'new_connection') {
                    msg = L === 'hi'
                        ? '‡§≤‡•â‡§ó‡§ø‡§® ‡§π‡•ã ‡§ó‡§Ø‡§æ! ‡§®‡§Ø‡§æ ‡§ï‡§®‡•á‡§ï‡•ç‡§∂‡§® ‡§≤‡§ó‡§æ‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§®‡•Ä‡§ö‡•á "üÜï Apply New Connection" ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç‡•§'
                        : 'Logged in! Press "üÜï Apply New Connection" button below.';
                } else if (intent.type === 'certificate') {
                    msg = L === 'hi'
                        ? '‡§≤‡•â‡§ó‡§ø‡§® ‡§π‡•ã ‡§ó‡§Ø‡§æ! ‡§™‡•ç‡§∞‡§Æ‡§æ‡§£‡§™‡§§‡•ç‡§∞ ‡§ï‡•á ‡§≤‡§ø‡§è "üìú Print Certificate" ‡§¨‡§ü‡§® ‡§¶‡§¨‡§æ‡§è‡§Ç‡•§'
                        : 'Logged in! Press "üìú Print Certificate" button.';
                } else {
                    const g = SCREEN_GUIDANCE['citizen-dashboard'];
                    msg = g[L] || g.en;
                }

                log(`üéØ Post-auth intent: ${intent.type}`);
                setLastReply(msg);
                await ttsSpeak(msg, langRef.current);
                if (isActiveRef.current) setStatus('listening');
                return;
            }

            const g = SCREEN_GUIDANCE[screen];
            if (g) {
                const text = g[langRef.current] || g.en;
                log(`üìç Screen guidance: ${screen}`);
                setLastReply(text);
                await ttsSpeak(text, langRef.current);
                if (isActiveRef.current) setStatus('listening');
            }
        };

        setTimeout(speakForScreen, 600);
    }, [screen, voiceMode, log, ttsSpeak]);

    // Route change ‚Üí cancel old + announce new
    useEffect(() => {
        if (!isActiveRef.current || !voiceMode) return;
        const currentPath = location.pathname;
        if (currentPath === lastRouteRef.current) return;

        const prevPath = lastRouteRef.current;
        lastRouteRef.current = currentPath;
        log(`üìç Route: ${prevPath} ‚Üí ${currentPath}`);

        // Cancel old speech
        window.speechSynthesis?.cancel();
        isSpeakingRef.current = false;
        bargedInRef.current = true;

        const guidance = getPageGuidance(currentPath, langRef.current);
        if (guidance) {
            const transition = prevPath ? (TRANSITION_PHRASE[langRef.current] || TRANSITION_PHRASE.en) : '';
            const fullText = transition + guidance;

            setTimeout(async () => {
                if (isActiveRef.current) {
                    bargedInRef.current = false;
                    setLastReply(fullText);
                    await ttsSpeak(fullText, langRef.current);
                    if (isActiveRef.current) setStatus('listening');
                }
            }, 400);
        }
    }, [location.pathname, voiceMode, log, ttsSpeak]);

    // ‚ïê‚ïê‚ïê ACTIVATE ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    const activateVoice = useCallback(async () => {
        if (isActiveRef.current) return;
        isActiveRef.current = true;
        setIsActive(true);
        setLastTranscript('');
        setLastReply('');
        setInterimText('');
        stopSpeaking();
        window.speechSynthesis?.cancel();
        restartCountRef.current = 0;
        rePromptCountRef.current = 0;
        lastRouteRef.current = window.location.pathname;
        lastScreenRef.current = screen;
        convStateRef.current = CONV_STATES.WAIT_PATH;
        pendingTranscriptRef.current = null;
        postAuthIntentRef.current = null;
        log('üü¢ Activated');

        const greeting = getInitialGreeting(langRef.current);
        setLastReply(greeting);
        setStatus('speaking');
        await ttsSpeak(greeting, langRef.current);

        if (isActiveRef.current) {
            log('üì¢ Listening');
            setStatus('listening');
            startRecognition();
            startRePromptTimer();
        }
    }, [screen, startRecognition, startRePromptTimer, log, ttsSpeak]);

    // ‚ïê‚ïê‚ïê DEACTIVATE ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

    const deactivateVoice = useCallback(() => {
        isActiveRef.current = false;
        setIsActive(false);
        setStatus('idle');
        setInterimText('');
        isSpeakingRef.current = false;
        processingRef.current = false;
        convStateRef.current = CONV_STATES.INITIAL;
        pendingTranscriptRef.current = null;
        postAuthIntentRef.current = null;
        window.speechSynthesis?.cancel();
        stopSpeaking();
        clearTimeout(silenceTimerRef.current);
        clearTimeout(rePromptTimerRef.current);
        try { recognitionRef.current?.abort(); } catch { }
        log('üî¥ Deactivated');
    }, [log]);

    // Auto-activate
    useEffect(() => {
        if (voiceMode && !isActiveRef.current && screen !== 'idle') {
            log('üîÑ Auto-activate');
            activateVoice();
        }
    }, [voiceMode, screen, activateVoice, log]);

    useEffect(() => {
        return () => {
            isActiveRef.current = false;
            try { recognitionRef.current?.abort(); } catch { }
            stopSpeaking();
            window.speechSynthesis?.cancel();
            clearTimeout(rePromptTimerRef.current);
            clearTimeout(silenceTimerRef.current);
        };
    }, []);

    const ctx = {
        voiceMode, isActive, status, lastTranscript, lastReply,
        activate: activateVoice, deactivate: deactivateVoice,
    };

    return (
        <VoiceContext.Provider value={ctx}>
            {children}

            {isActive && (
                <div className={`vo-bar vo-bar-${status}`}>
                    <div className="vo-bar-left">
                        {status === 'listening' && (
                            <>
                                <div className="vo-pulse" />
                                <span className="vo-bar-label">
                                    {interimText
                                        ? `"${interimText.substring(0, 60)}${interimText.length > 60 ? '...' : ''}"`
                                        : (lang === 'hi' ? 'üéß ‡§¨‡•ã‡§≤‡§ø‡§è...' : 'üéß Speak...')}
                                </span>
                            </>
                        )}
                        {status === 'processing' && (
                            <>
                                <div className="vo-spinner" />
                                <span className="vo-bar-label">
                                    {lastTranscript
                                        ? `"${lastTranscript.substring(0, 50)}..."`
                                        : (lang === 'hi' ? 'üß† ‡§∏‡§Æ‡§ù ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Å...' : 'üß† Thinking...')}
                                </span>
                            </>
                        )}
                        {status === 'speaking' && (
                            <>
                                <div className="vo-waves">
                                    {[...Array(4)].map((_, i) => (
                                        <div key={i} className="vo-wave" style={{ animationDelay: `${i * 0.12}s` }} />
                                    ))}
                                </div>
                                <span className="vo-bar-reply">
                                    {lastReply?.substring(0, 90)}{lastReply?.length > 90 ? '...' : ''}
                                </span>
                            </>
                        )}
                    </div>
                    <button
                        className="vo-bar-close"
                        title={status === 'speaking' ? 'Skip' : 'Stop voice'}
                        onClick={status === 'speaking'
                            ? () => {
                                window.speechSynthesis?.cancel();
                                isSpeakingRef.current = false;
                                bargedInRef.current = true;
                                setStatus('listening');
                                setTimeout(() => restartRecognition(), 200);
                            }
                            : deactivateVoice}
                    >
                        {status === 'speaking' ? '‚è≠' : '‚úï'}
                    </button>
                </div>
            )}
        </VoiceContext.Provider>
    );
});

export default VoiceAgent;
